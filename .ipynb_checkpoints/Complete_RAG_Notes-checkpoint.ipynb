{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114fffa4-1951-40d0-8bdf-3f848e4522ff",
   "metadata": {},
   "source": [
    "# âœ… **What is RAG**\n",
    "\n",
    "---\n",
    "\n",
    "**RAG** stands for **Retrieval-Augmented Generation**.  \n",
    "It combines **LLMs** with **external knowledge sources** to generate more accurate and grounded responses.\n",
    "\n",
    " **Purpose**:  \n",
    "Retrieve relevant information from a knowledge base and feed it to the LLM for context-aware generation.\n",
    "\n",
    "---\n",
    "\n",
    "# Components of RAG\n",
    "\n",
    "RAG typically uses **four key components**:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Document Loaders\n",
    "\n",
    "- **Function**: Load data from various sources (PDFs, websites, Notion, files, etc.)\n",
    "- **Purpose**: Convert raw, unstructured content into a structured list of `Document` objects used in LangChain pipelines.\n",
    "\n",
    "- **Examples**:\n",
    "\n",
    "- `PyPDFLoader` â€“ Loads content from PDF files  \n",
    "- `WebBaseLoader` â€“ Loads content from websites  \n",
    "- `NotionLoader` â€“ Loads content from Notion pages  \n",
    "- `TextLoader` â€“ Loads plain `.txt` files  \n",
    "- `DirectoryLoader` â€“ Loads all documents from a directory (can combine with any loader type)  \n",
    "- `CSVLoader` â€“ Loads data from CSV files (as documents per row or column)  \n",
    "- **Custom Document Loader** â€“ You can subclass `BaseLoader` to define your own loader for any data source (e.g., internal tools, APIs)\n",
    "\n",
    "---\n",
    "\n",
    "###  Useful patterns in Directory Loader\n",
    "| **Glob Pattern** | **What It Loads**                                          |\n",
    "| ---------------- | ---------------------------------------------------------- |\n",
    "| `\"**/*.txt\"`     | All `.txt` files in all folders and subfolders (recursive) |\n",
    "| `\"*.pdf\"`        | All `.pdf` files in the root directory only                |\n",
    "| `\"data/*.csv\"`   | All `.csv` files inside the `data/` folder (not recursive) |\n",
    "| `\"**/*\"`         | All files of any type in all folders and subfolders        |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Text Splitters\n",
    "- **Function**: Split large texts into smaller, manageable chunks.\n",
    "- **Purpose**: Improve chunk-level retrieval accuracy and avoid context overflow.\n",
    "\n",
    "- **Examples**:\n",
    "| **Splitter Type**        | **What It Does**                      | **Examples**                                            |\n",
    "| ------------------------ | ------------------------------------- | ------------------------------------------------------- |\n",
    "| Length-Based             | Splits by size (tokens/chars)         | Fixed-size, overlapping, recursive character splitters  |\n",
    "| Text Structure-Based     | Uses natural text layout              | Sentence-based, paragraph-based, custom delimiter split |\n",
    "| Document Structure-Based | Uses formal layout structure          | Headings in Markdown, PDF sections, slide breaks        |\n",
    "| Semantic Meaning-Based   | Splits where topic or meaning changes | Embedding-based splits, semantic chunkers               |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Vector Databases\n",
    "- **Function**: Store text chunks as **embeddings** (vector representations).\n",
    "- **Purpose**: Enable efficient **similarity search** based on vector distances.\n",
    "- **Examples**:\n",
    "  - FAISS  \n",
    "  - Pinecone  \n",
    "  - Chroma  \n",
    "  - Weaviate  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Retrievers\n",
    "A **Retriever** is a component in a RAG (Retrieval-Augmented Generation) system that:\n",
    "- Accepts a **query** from the user.\n",
    "- Searches a **data source** to find relevant documents.\n",
    "- Returns these documents to be used by a language model for generating answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Retrievers\n",
    "\n",
    "Retrievers can be classified in two main ways:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Based on **Data Source**\n",
    "\n",
    "These determine **where** the retriever looks for information.\n",
    "\n",
    "- ðŸ”¸ **Wikipedia Retriever**  \n",
    "  Uses Wikipedia articles as the source of knowledge.\n",
    "\n",
    "- ðŸ”¸ **Vector Store Retriever**  \n",
    "  Retrieves documents based on vector similarity (e.g., using document embeddings).\n",
    "\n",
    "- ðŸ”¸ **arXiv Retriever**  \n",
    "  Searches academic papers on the arXiv platform.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Based on **Search Strategy**\n",
    "\n",
    "These define **how** the retriever searches and ranks information.\n",
    "\n",
    "- ðŸ”¹ **MMR (Maximal Marginal Relevance)**  \n",
    "  Balances relevance and diversity to avoid redundant results.\n",
    "\n",
    "- ðŸ”¹ **Multi-query Retrieval**  \n",
    "  Uses multiple reworded versions of a query to gather broader or more accurate results.\n",
    "\n",
    "- ðŸ”¹ **Contextual Compression**  \n",
    "  Compresses the retrieved documents before passing them to the model (e.g., by summarizing or trimming).\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **load() vs lazy_load()**\n",
    "\n",
    "- `load()`  \n",
    "  - **Eager loading**  \n",
    "  - Reads and processes **all documents at once**  \n",
    "  - Returns a list of `Document` objects  \n",
    "  - Good for small/medium datasets\n",
    "\n",
    "- `lazy_load()`  \n",
    "  - **Lazy (streamed) loading**  \n",
    "  - Returns a **generator** instead of a list  \n",
    "  - Memory efficient for large datasets  \n",
    "  - Useful when you want to process documents one-by-one or in batches\n",
    "\n",
    "```python\n",
    "# Example\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"notes.txt\")\n",
    "docs = loader.load()         # Loads all documents immediately\n",
    "docs_lazy = loader.lazy_load()  # Loads documents one-by-one as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8120a4-8c1a-4ba0-972d-782d42141bfe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# âœ… **Understanding RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "RAG combines the power of:\n",
    "- **Information Retrieval**  \n",
    "- **Text Generation**\n",
    "\n",
    "> Goal: Provide **accurate**, **context-aware** responses by enhancing LLMs with external knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "# Overview of the Process\n",
    "\n",
    "\n",
    "      Query       Context\n",
    "        â¬‡           â¬‡\n",
    "         \\         /\n",
    "          âž¡ï¸   Prompt\n",
    "                  â¬‡\n",
    "                 LLM\n",
    "                  â¬‡\n",
    "               Response\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 4 Key Phases of RAG\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Indexing\n",
    "\n",
    "> *Preparation step (done before queries are made)*\n",
    "\n",
    "  - Raw documents (PDFs, articles, notes, etc.) are:\n",
    "  - Split into **chunks**\n",
    "  - Converted into **embeddings**\n",
    "  - Stored in a **vector database** (like FAISS or Chroma)\n",
    "\n",
    "Enables fast, efficient similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Retrieval\n",
    "\n",
    "> *Happens when the user submits a query*\n",
    "\n",
    "- The system:\n",
    "  - Converts the query to an embedding\n",
    "  - Retrieves **relevant document chunks** from the vector DB\n",
    "- These retrieved chunks form the **Context**\n",
    "\n",
    "- Supplies the LLM with real-world/domain-specific knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Augmentation\n",
    "\n",
    "> *The core idea of RAG*\n",
    "\n",
    "- The system merges:\n",
    "  - The user **Query**\n",
    "  - The **Context** from retrieval\n",
    "- Together, they form a **Prompt** for the LLM\n",
    "\n",
    "- Gives the LLM extra context for grounded answers.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Generation\n",
    "\n",
    "> *Final step: answer creation*\n",
    "\n",
    "- The LLM receives the **augmented prompt**\n",
    "- Generates a **context-aware, relevant response**\n",
    "- Response is returned to the user\n",
    "\n",
    "- Powered by both the LLM's knowledge + retrieved data.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary \n",
    "\n",
    "| Step             | What Happens                                         | Diagram Block  |\n",
    "|------------------|------------------------------------------------------|----------------|\n",
    "| **Indexing**     | Prepare documents: chunk â†’ embed â†’ store             | *Implied*      |\n",
    "| **Retrieval**    | Fetch top relevant chunks from vector store          | Context        |\n",
    "| **Augmentation** | Combine Query + Context into LLM prompt              | Prompt         |\n",
    "| **Generation**   | LLM produces final answer from the prompt            | LLM â†’ Response |\n",
    "\n",
    "---\n",
    "\n",
    "**RAG = Smart retrieval + powerful generation.**  \n",
    "It makes LLMs more useful, factual, and personalized.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
